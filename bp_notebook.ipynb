{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42ff2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, instrument, note, chord, stream\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from collections import Counter\n",
    "from fractions import Fraction\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import sklearn\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1147f32",
   "metadata": {},
   "source": [
    "parse_MIDI.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4654cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_file(file_name):\n",
    "\n",
    "    midi_sample = None\n",
    "    try:\n",
    "        midi_sample = converter.parse(file_name)\n",
    "    except OSError as e:\n",
    "        print('\\nERROR loading MIDI file in load_midi_file()')\n",
    "        print(e)\n",
    "        quit()\n",
    "    return midi_sample\n",
    "\n",
    "\n",
    "def parse_midi_file(folder_path):\n",
    "\n",
    "    notes_piano = []\n",
    "    metadata_piano = {\n",
    "        \"note_count\": 0,\n",
    "        \"chord_count\": 0,\n",
    "        \"rest\": 0,\n",
    "        \"else_count\": 0\n",
    "    }\n",
    "\n",
    "    all_names = []\n",
    "    print('Parsing PIANO...')\n",
    "\n",
    "    for oneFile in os.listdir(folder_path):\n",
    "        if oneFile.endswith(\".mid\"):\n",
    "            all_names.append(oneFile)\n",
    "            midi_file = folder_path+oneFile\n",
    "            midi_sample = load_midi_file(midi_file)\n",
    "            instruments = instrument.partitionByInstrument(midi_sample)\n",
    "\n",
    "            for part in instruments.parts:\n",
    "\n",
    "                if 'Piano' in str(part):\n",
    "                    # print('parsing PIANO', part)\n",
    "\n",
    "                    notes_to_parse = part.recurse()\n",
    "                    last_offset = 0\n",
    "\n",
    "                    # note  -> n_pitch_quarterLength_deltaOffset\n",
    "                    # chord -> c_pitch_quarterLength_deltaOffset\n",
    "                    # rest  -> r_quarterLength_deltaOffset\n",
    "\n",
    "                    for element in notes_to_parse:\n",
    "                        if isinstance(element, note.Note):\n",
    "                            delta_offset = Fraction(element.offset) - last_offset\n",
    "                            last_offset = Fraction(element.offset)\n",
    "\n",
    "                            notes_piano.append('n_'+str(element.pitch)+'_'+str(element.duration.quarterLength)+'_'+str(delta_offset))\n",
    "                            metadata_piano[\"note_count\"] += 1\n",
    "\n",
    "                        elif isinstance(element, chord.Chord):\n",
    "                            delta_offset = Fraction(element.offset) - last_offset\n",
    "                            last_offset = Fraction(element.offset)\n",
    "\n",
    "                            chord_ = '.'.join(str(n) for n in element.pitches)\n",
    "                            notes_piano.append('c_' + chord_ + '_' + str(element.duration.quarterLength)+'_'+str(delta_offset))\n",
    "                            metadata_piano[\"chord_count\"] += 1\n",
    "\n",
    "                        elif isinstance(element, note.Rest):\n",
    "                            delta_offset = Fraction(element.offset) - last_offset\n",
    "                            last_offset = Fraction(element.offset)\n",
    "\n",
    "                            notes_piano.append('r_' + str(element.duration.quarterLength)+'_'+str(delta_offset))\n",
    "                            metadata_piano[\"rest\"] += 1\n",
    "                        else:\n",
    "                            metadata_piano[\"else_count\"] += 1\n",
    "    print('Parsing PIANO finished')\n",
    "    print('Parsed MIDI dataset:\\n', all_names)\n",
    "    print('Parsed MIDI dataset length:', len(all_names))\n",
    "    return notes_piano, metadata_piano\n",
    "\n",
    "\n",
    "def average_f(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "   return [value for value in the_list if value != val]\n",
    "\n",
    "\n",
    "def cut_notes(uncut_notes, metadata, cuts):\n",
    "    notes = uncut_notes\n",
    "    stop_flag = False\n",
    "\n",
    "    for i in range(cuts):\n",
    "        notes_count = Counter(notes)\n",
    "        Recurrence = list(notes_count.values())\n",
    "\n",
    "        pitchnames = set(notes)\n",
    "        note_to_int_before = dict((note_var, number) for number, note_var in enumerate(pitchnames))\n",
    "        avg_r = round(average_f(Recurrence), 2)\n",
    "\n",
    "        print('\\nnumber of notes before:', len(notes))\n",
    "        print('number of unique notes before:', len(note_to_int_before))\n",
    "        print(\"average recurrence for a note in notes:\", avg_r)\n",
    "        print(\"most frequent note in notes appeared:\", max(Recurrence), \"times\")\n",
    "        print(\"least frequent note in notes appeared:\", min(Recurrence), \"time/s\")\n",
    "\n",
    "        # if average recurrence is more then a 100, @param avg_r is set to 100 and this will be final eliminating\n",
    "        if round(avg_r) >= 100:\n",
    "            avg_r = 100\n",
    "            stop_flag = True\n",
    "\n",
    "        # getting a list of elements that appear less then avarage element does\n",
    "        less_avg_note = []\n",
    "        cn_items = notes_count.items()\n",
    "        for index, (key, value) in enumerate(cn_items):\n",
    "            if value < round(avg_r):\n",
    "                m = key\n",
    "                less_avg_note.append(m)\n",
    "        print(f'number of notes occuring less than {round(avg_r)} times:', len(less_avg_note))\n",
    "\n",
    "        # eleminating those elements and adjusting metadata accordingly\n",
    "        for element in notes:\n",
    "            if element in less_avg_note:\n",
    "                len_before = len(notes)\n",
    "                notes = remove_values_from_list(notes, element)\n",
    "                elements_removed = len_before - len(notes)\n",
    "                if element[:1] == 'n':\n",
    "                    metadata['note_count'] -= elements_removed\n",
    "                elif element[:1] == 'c':\n",
    "                    metadata['chord_count'] -= elements_removed\n",
    "                elif element[:1] == 'r':\n",
    "                    metadata['rest'] -= elements_removed\n",
    "\n",
    "        print(\"length of notes after the elemination :\", len(notes))\n",
    "        if stop_flag:\n",
    "            break\n",
    "\n",
    "    notes_count = Counter(notes)\n",
    "    Recurrence = list(notes_count.values())\n",
    "\n",
    "    avg_r = round(average_f(Recurrence), 2)\n",
    "\n",
    "    print(\"\\naverage recurrence for a note in notes:\", avg_r)\n",
    "    print(\"most frequent note in notes appeared:\", max(Recurrence), \"times\")\n",
    "    print(\"least frequent note in notes appeared:\", min(Recurrence), \"time/s\")\n",
    "\n",
    "    del notes_count, Recurrence, pitchnames, note_to_int_before, less_avg_note\n",
    "    gc.collect()\n",
    "\n",
    "    return notes, metadata\n",
    "\n",
    "\n",
    "def mapping(uncut_notes, metadata, sequence_len, cuts):\n",
    "\n",
    "    if cuts > 0:\n",
    "        notes, new_metadata = cut_notes(uncut_notes, metadata, cuts)\n",
    "    else:\n",
    "        notes, new_metadata = uncut_notes, metadata\n",
    "\n",
    "    sequence_length = sequence_len\n",
    "    pitchnames = set(notes)\n",
    "    note_to_int = dict((note_var, number) for number, note_var in enumerate(pitchnames))\n",
    "\n",
    "    nn_input = []\n",
    "    nn_output = []\n",
    "\n",
    "    for i in range(0, len(notes) - sequence_length):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "\n",
    "        nn_input.append([note_to_int[one_note] for one_note in sequence_in])\n",
    "        nn_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    input_count = len(nn_input)\n",
    "    mapped_n_count = float(len(note_to_int))\n",
    "\n",
    "    # reshape the input so it's compatible with LSTM layers\n",
    "    nn_input = np.reshape(nn_input, (input_count, sequence_length, 1))\n",
    "\n",
    "    # normalize input values\n",
    "    nn_input = nn_input / mapped_n_count\n",
    "\n",
    "    # set output values so it's compatible with LSTM layers\n",
    "    nn_output = np_utils.to_categorical(nn_output)\n",
    "\n",
    "    # print metadata about notes after removing\n",
    "    info_print_out(new_metadata, len(note_to_int))\n",
    "\n",
    "    return nn_input, nn_output, note_to_int, pitchnames\n",
    "\n",
    "\n",
    "def info_print_out(metadata, unique_elements_count):\n",
    "    all_count = metadata['note_count'] + metadata['chord_count'] + metadata['rest']\n",
    "    print('\\n')\n",
    "    print('_________________________________________________')\n",
    "    print('\\n')\n",
    "    print('number of all elements:      ', all_count)\n",
    "    print('notes:    ', metadata['note_count'])\n",
    "    print('chords:   ', metadata['chord_count'])\n",
    "    print('rests:    ', metadata['rest'])\n",
    "    print('number of unique elements:   ', unique_elements_count)\n",
    "    print('\\n')\n",
    "    print('_________________________________________________')\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def parse_MIDI_init(folder_path, sequence_length, cuts):\n",
    "    notes_and_chords, metadata_p = parse_midi_file(folder_path)                                                         # parse MIDI file\n",
    "    lstm_input, lstm_output, notes_to_int, pitch_names = mapping(notes_and_chords, metadata_p, sequence_length, cuts)   # mapping MIDI file parts\n",
    "\n",
    "    # lstm_input_shuffled, lstm_output_shuffled = sklearn.utils.shuffle(lstm_input, lstm_output)                        # shuffling input and output simultaneously\n",
    "    pitch_names_len = len(pitch_names)\n",
    "\n",
    "    # del lstm_input\n",
    "    # del lstm_output\n",
    "    del notes_and_chords\n",
    "    del metadata_p\n",
    "    del pitch_names\n",
    "    gc.collect()\n",
    "\n",
    "    return lstm_input, lstm_output, notes_to_int, pitch_names_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5a80d",
   "metadata": {},
   "source": [
    "train_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28970a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(nn_input, n_pitch):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(\n",
    "        256,\n",
    "        input_shape=(nn_input.shape[1], nn_input.shape[2]),\n",
    "        return_sequences=True,\n",
    "    ))\n",
    "    lstm_model.add(Dropout(0.6))\n",
    "    lstm_model.add(LSTM(256, return_sequences=True))\n",
    "    lstm_model.add(Dropout(0.6))\n",
    "    lstm_model.add(LSTM(256))\n",
    "    lstm_model.add(Dropout(0.6))\n",
    "\n",
    "    lstm_model.add(Dense(256))\n",
    "    lstm_model.add(Dropout(0.6))\n",
    "\n",
    "    lstm_model.add(Dense(n_pitch))\n",
    "    lstm_model.add(Activation('sigmoid'))\n",
    "    lstm_model.compile(optimizer='Adam', loss='categorical_crossentropy')\n",
    "\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "def load_weight_to_model(empt_model, weight):\n",
    "    filepath = f'weights\\\\toLoadWeights\\\\{weight}.hdf5'\n",
    "    try:\n",
    "        empt_model.load_weights(filepath)\n",
    "    except OSError as e:\n",
    "        print('\\nERROR loading weights file in load_weight_to_model()')\n",
    "        print(e)\n",
    "        quit()\n",
    "    return empt_model\n",
    "\n",
    "\n",
    "def train_lstm(nn, nn_input, nn_output, epochs, batch_size):\n",
    "\n",
    "    filepath = \"weights\\\\weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # print('X[0]: ', nn_input[0])\n",
    "    # print('argmax y[0]: ', np.argmax(nn_output[0]))\n",
    "    # print('argmax y[0] / 18: ', np.argmax(nn_output[0]) / float(18))  # 18 je cislo mapped_notes\n",
    "\n",
    "    data = nn.fit(nn_input, nn_output, epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
    "\n",
    "    # draw a graph of loss during training\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    plt.plot(data.history['loss'], label=f'Loss hodnota', lw=2)\n",
    "    plt.title('Trénovanie celého datasetu')\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                     box.width, box.height * 0.9])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.135),\n",
    "              fancybox=True, shadow=True, ncol=5)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "    fig.savefig('tests\\\\cely-dataset-bs64_.pdf')\n",
    "    plt.clf()\n",
    "    \n",
    "    return nn\n",
    "\n",
    "\n",
    "def train_model_init(lstm_input, lstm_output, pitch_names_len, epochs, batch_size, model_training):\n",
    "\n",
    "    empty_model = create_lstm_model(lstm_input, pitch_names_len)                        # load layers of NN to model\n",
    "\n",
    "    if model_training[\"bool\"]:\n",
    "        model = train_lstm(empty_model, lstm_input, lstm_output, epochs, batch_size)    # train NN\n",
    "    else:\n",
    "        model = load_weight_to_model(empty_model, model_training[\"weight\"])             # load weights to model\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8b9ab",
   "metadata": {},
   "source": [
    "generate_music.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a322b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of this function : https://stackoverflow.com/questions/1806278/convert-fraction-to-float\n",
    "def convert_to_float(frac_str):\n",
    "    try:\n",
    "        return float(frac_str)\n",
    "    except ValueError:\n",
    "        num, denom = frac_str.split('/')\n",
    "        try:\n",
    "            leading, num = num.split(' ')\n",
    "            whole = float(leading)\n",
    "        except ValueError:\n",
    "            whole = 0\n",
    "        frac = float(num) / float(denom)\n",
    "        return whole - frac if whole < 0 else whole + frac\n",
    "\n",
    "\n",
    "def create_midi_file(output, mapping_keys, index, new_file_name):\n",
    "\n",
    "    unmapped_from_int = []\n",
    "    converted = []\n",
    "    notes = []\n",
    "    offset = 0\n",
    "\n",
    "    metadata = {\n",
    "        \"note\": 0,\n",
    "        \"chord\": 0,\n",
    "        \"rest\": 0\n",
    "    }\n",
    "\n",
    "    # # unmapping notes, chores and rests from output integers\n",
    "    for element in output:\n",
    "        for key in mapping_keys:\n",
    "            if int(mapping_keys.get(key)) == element:\n",
    "                unmapped_from_int.append(key)\n",
    "                break\n",
    "\n",
    "    # creating note, chores and rest objects\n",
    "    for element in unmapped_from_int:\n",
    "        if 'n_' in element:                         # note\n",
    "            element = element[2:]                                                       # cut the 'n_' mark\n",
    "            offset += Fraction(element.split('_')[2])\n",
    "            note_ = note.Note(element.split('_')[0])                                    # creating note\n",
    "            note_.duration.quarterLength = convert_to_float(element.split('_')[1])      # adding duration quarterLength\n",
    "            note_.offset = offset                                                       # adding offset\n",
    "            note_.storedInstrument = instrument.Piano()\n",
    "            converted.append(note_)                                                     # appending final array\n",
    "            metadata['note'] = metadata['note'] + 1\n",
    "\n",
    "        elif 'c_' in element:                       # chord\n",
    "            element = element[2:]                                                       # cut the 'c_' mark\n",
    "            offset += Fraction(element.split('_')[2])\n",
    "            notes_in_chord = element.split('_')[0].split('.')                           # gettig notes as string from element\n",
    "\n",
    "            notes.clear()\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(current_note)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "\n",
    "            chord_ = chord.Chord(notes)                                                 # creating chord form notes\n",
    "            chord_.duration.quarterLength = convert_to_float(element.split('_')[1])     # adding duration quarterLength\n",
    "            chord_.offset = offset                                                      # adding offset\n",
    "            converted.append(chord_)                                                    # appending final array\n",
    "            metadata['chord'] = metadata['chord'] + 1\n",
    "\n",
    "        elif 'r_' in element:                       # rest\n",
    "            element = element[2:]                                                       # cut the 'r_' mark\n",
    "            offset += Fraction(element.split('_')[1])\n",
    "            rest_ = note.Rest()                                                         # creating rest\n",
    "            rest_.duration.quarterLength = convert_to_float(element.split('_')[0])      # adding duration quarterLength\n",
    "            rest_.offset = offset                                                       # adding offset\n",
    "            rest_.storedInstrument = instrument.Piano()\n",
    "            converted.append(rest_)                                                     # appending final array\n",
    "            metadata['rest'] = metadata['rest'] + 1\n",
    "\n",
    "    print(f'\\nElements of newly generated music with index={index}: ', metadata)\n",
    "\n",
    "    try:\n",
    "        midi_stream = stream.Stream(converted)\n",
    "        midi_stream.write('midi', fp='midi_samples\\\\outputs\\\\' + f'{new_file_name}' + str(index) + '.mid') \n",
    "        print('Created new MIDI file')\n",
    "        midi_stream.show('midi')\n",
    "    except OSError as e:\n",
    "        print('\\nERROR creating MIDI file in create_midi_file()')\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def generate_music(nn_model, nn_input, mapped_notes, length):\n",
    "\n",
    "    generated_music = []\n",
    "    sequence_len = len(nn_input[0])\n",
    "    mapped_notes_count = len(mapped_notes)\n",
    "    init_input = np.random.randint(0, len(nn_input) - 1)\n",
    "    pattern = nn_input[init_input]\n",
    "\n",
    "    note_input = np.array(pattern).reshape((1, sequence_len, 1))\n",
    "    note_input = note_input / float(mapped_notes_count)             # works without it, but generated music is significantly better\n",
    "\n",
    "    for i in range(length):\n",
    "        note_output = nn_model.predict(note_input, verbose=0)\n",
    "        note_output_max = np.argmax(note_output)\n",
    "        generated_music.append(note_output_max)\n",
    "\n",
    "        pattern = np.append(pattern, note_output_max / float(mapped_notes_count))\n",
    "        pattern = pattern[1:sequence_len + 1]\n",
    "\n",
    "        note_input = np.array(pattern).reshape((1, sequence_len, 1))\n",
    "\n",
    "    return generated_music\n",
    "\n",
    "\n",
    "def generate_music_init(model, lstm_input, notes_to_int, length, index, new_file_name):\n",
    "    new_music = generate_music(model, lstm_input, notes_to_int, length)     # predict new music\n",
    "    create_midi_file(new_music, notes_to_int, index, new_file_name)         # save new music to MIDI file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23a802",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c10286",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    start_time = time.time()\n",
    "    print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    sequence_length = int(config[\"sequence_length\"])\n",
    "    midi_files_folder = 'midi_samples\\\\' + config[\"dataset_folder\"] + '\\\\'\n",
    "    cuts = int(config[\"cuts\"])\n",
    "    epochs = int(config[\"epochs\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "    new_music_length = int(config[\"new_music_length\"])\n",
    "    tracks_to_generate = int(config[\"tracks_to_generate\"])\n",
    "    new_music_file_name = config[\"new_music_file_name\"]\n",
    "    model_training = config[\"training\"]\n",
    "\n",
    "    lstm_input, lstm_output, notes_to_int, pitch_names_len = parse_MIDI_init(midi_files_folder, sequence_length, cuts)\n",
    "    model = train_model_init(lstm_input, lstm_output, pitch_names_len, epochs, batch_size, model_training)\n",
    "\n",
    "    for i in range(tracks_to_generate):\n",
    "        generate_music_init(model, lstm_input, notes_to_int, new_music_length, i, new_music_file_name)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Time:', int(end_time - start_time), 's')\n",
    "    print('\\n--- END ---\\n')\n",
    "    # model.summary()\n",
    "\n",
    "except OSError as e:\n",
    "    print('\\nERROR loading config file in main.py')\n",
    "    print(e)\n",
    "    quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
